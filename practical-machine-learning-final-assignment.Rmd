---
title: "Human Activity Recognition - Predicting how well weight lifting excercises were performed using sensory data"
subtitle: "Practical Machine Learning Final Assignment"
author: "atomasovszky"
date: "4/27/2019"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    messages = FALSE,
    dpi = 300,
    out.width = "600pt",
    fig.align = "center"
)

source("libraries_final_assignment.R")

train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- fread(train_url)
test  <- fread(test_url)
```


&nbsp;
&nbsp;



## Synopsis

The goal of this work is to use on-body sensor data to predict whether a particular excercise is performed correctly.
The 6 participants of the study our data is coming from were equippped with movement sensors on their arm, forearm, a belt sensor on their waist and one on their dumbell, and they've been asked to perform weight lifting in a proper way (classe = A), and with some common mistakes (classe = {B, C, D, E}).


&nbsp;
&nbsp;


## Feature selection

Besides the obvious ones (ids, timestamps, etc.) we also left summary variables out as they have missing values for most of the records.

```{r feature_selection}
summary_vars <- grep(
    pattern = "^kurtosis|^skewness|^max|^min|^amplitude|^var|^avg|^stddev", 
    x       = names(train),
    value   = TRUE
)

variables_to_omit <- c(
    summary_vars,
    grep("window|V1|user|timestamp", names(train), value = TRUE)
)

train_clean <- train[, .SD, .SDcols = setdiff(names(train), variables_to_omit)]
```


&nbsp;
&nbsp;


## Checking the distribution of features

The three boxplots included here shows that there are a few outliers in the dataset.
These come from two cases, id 5373, and 9274. 
We omitted these during the training.

```{r feature_distribution_check}
walk(setdiff(names(train_clean), "classe"), ~{
    filename <- glue("figures/{.x}.png")
    if (!file.exists(filename)) {
        ggplot(train_clean, aes_string("classe", .x, fill = "classe")) +
            geom_boxplot(alpha = .75)
        ggsave(filename)
    }
})

train_clean %>%
    .[, .(magnet_dumbbell_y, gyros_dumbbell_x, gyros_forearm_x, classe)] %>%
    .[, lapply(.SD, as.double), classe] %>%
    melt(id.vars = "classe")  %>%
    ggplot(aes(classe, value, fill = classe)) +
        geom_boxplot(alpha = .75) +
        facet_wrap(~variable, nrow = 1, scales = "free_y") +
        labs(y = "")

outliers <- train[magnet_dumbbell_y < -2000 | gyros_forearm_x < -20][, V1]
outliers
train_clean <- train_clean[magnet_dumbbell_y > -2000 | gyros_forearm_x > -20]
```


&nbsp;
&nbsp;


## Training the model using cross validation

We chose to use random forest for the prediction task over CART and rpart as it corrects for the latter two methods' tendency to overfitting and thus we expected to reach a better bias-variance tradeoff.
In order to estimate the out-of-sample error (accuracy) we used repeated k-fold cross validation with 10 folds and 3 repetitions for each of them.
We estimated the error by the average of the model's accuracy on these 30 folds. 

&nbsp;

As the training of the random forest model would have took too long, we only used 5000 randomly selected cases from the original training dataset.
We also set the `ntrees` parameter to 50, so the algorithm created only 50 bootstrap samples and trees using them.

```{r training, cache = TRUE}
train_clean[, classe := as.factor(classe)]

set.seed(20190428)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)
cv_control <- trainControl(method = "repeatedcv", 10, 3, allowParallel = TRUE)

in_train_clean_sample <- sample(1:nrow(train_clean), 5000)
train_clean_sample <- train_clean[in_train_clean_sample]

rf_train <- train(
    form      = classe ~ .,
    data      = train_clean_sample,
    method    = "rf",
    ntree     = 50,
    trControl = cv_control
)

stopImplicitCluster()
registerDoSEQ()

rf_train
```

&nbsp;

Even with the 5000 subsample we see that we estimate the out-of-sample accuracy of the best model to be 97.2%.

&nbsp;

### In sample error
```{r train_confusion}
confusionMatrix(
    rf_train$finalModel$predicted, 
    train_clean_sample[, classe]
)
```


&nbsp;
&nbsp;


If we predict `classe` for the 14,622 cases we left out during the training, we can see that the accuracy is still around 97%.

### Out-of-sample error
```{r oos_confusion}
confusionMatrix(
    predict(rf_train$finalModel, train_clean[-in_train_clean_sample]),
    train_clean[-in_train_clean_sample, classe]
)
```


&nbsp;
&nbsp;


## Predicing on test set
```{r test_prediction}
predict(
    rf_train$finalModel, 
    test[, .SD, .SDcols = setdiff(names(test), variables_to_omit)]
)
```


&nbsp;
&nbsp;


```{r copy_html, include = FALSE}
file.copy(
    "practical-machine-learning-final-assignment.html", 
    "index.html", 
    overwrite = TRUE
)
```
